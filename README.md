# RL Robot Navigation in a 2D Grid-System

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PyQt5](https://img.shields.io/badge/PyQt5-5.15.9-green.svg)
![License](https://img.shields.io/badge/license-MIT-orange.svg)

A PyQt5-based desktop application for visualizing and training a robot using Q-learning reinforcement learning algorithm in a grid-world environment.

## ğŸ“¸ Demo

*Main application window showing grid environment and training metrics and real-time training visualization*

https://github.com/user-attachments/assets/aaf340c3-5326-4e12-aa42-291f7163e925


## âœ¨ Features

- **Interactive Grid Environment**: 10x10 customizable grid with random obstacles
- **Real-time Training Visualization**: Watch the agent learn optimal paths dynamically
- **6 Analytics Charts**: 
  - Episode Rewards
  - Average Reward Trend (Smoothed)
  - Q-Value Statistics (Min/Max/Mean)
  - Epsilon Decay Over Time
  - Steps to Goal per Episode
  - Success Rate Over Time
- **Adjustable Hyperparameters**: Modify learning rate, discount factor, and epsilon on-the-fly
- **Model Persistence**: Save and load trained Q-tables
- **Export Capabilities**: Save all training metrics as PDF or individual PNG files


## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Setup
1. **Clone the repository:**
```bash
git clone https://github.com/pulkitsu/RL-Robot-Navigation-in-a-2D-Grid-System.git
cd RL-Robot-Navigation-in-a-2D-Grid-System
```
2. **Create a virtual environment (recommended):**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```
3. **Install dependencies:**
```bash
pip install -r requirements.txt
```


## ğŸ’» Usage

### Basic Usage
Run the application:
```bash
python main.py
```
### Training the Agent
1. **Start Training**: Click the "Start Training" button to begin Q-learning
2. **Monitor Progress**: Watch the grid visualization and real-time charts
3. **Adjust Parameters**: Modify hyperparameters during training in the right panel
4. **Pause/Resume**: Use "Pause" button to temporarily stop training
5. **Visualize Learned Policy**: Click "Visualize Path" to see the optimal route


## âš™ï¸ Configuration

### Hyperparameters
| Parameter | Symbol | Range | Default | Description |
|-----------|--------|-------|---------|-------------|
| **Learning Rate** | Î± | 0.01 - 1.0 | 0.1 | Controls how much new information overrides old knowledge |
| **Discount Factor** | Î³ | 0.0 - 1.0 | 0.95 | Determines importance of future rewards vs immediate rewards |
| **Epsilon** | Îµ | 0.0 - 1.0 | 0.1 | Exploration rate (higher = more random exploration) |
| **Steps per Update** | - | 1 - 100 | 10 | Number of training steps before GUI refresh |

### Environment Configuration
You can modify the environment by changing these parameters in `main.py`:
```python
# In RobotNavigationGUI.__init__()
self.env = GridEnvironment(
    grid_size=10,          # Size of the grid (10x10)
    obstacle_density=0.2   # 20% of cells are obstacles
)
```

### Reward Structure

- **Reach Goal**: +100
- **Hit Obstacle/Wall**: -1
- **Normal Step**: -0.1 (encourages shorter paths)

## ğŸ“š Documentation

### How Q-Learning Works

Q-Learning is a model-free reinforcement learning algorithm that learns the value of actions in different states. The agent updates its Q-table using:

```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max(Q(s',a')) - Q(s,a)]
```

Where:
- `s` = current state
- `a` = action taken
- `r` = reward received
- `s'` = next state
- `Î±` = learning rate
- `Î³` = discount factor


## ğŸ“¦ Project Structure

```
RL-Robot-Navigation-in-a-2D-Grid-System/
â”œâ”€â”€ main.py              # Main application file (all code)
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ screenshots/        # Demo images and GIFs
â”‚   â”œâ”€â”€ main_window.png
â”‚   â””â”€â”€ training_demo.gif
â”œâ”€â”€ saved_models/       # Directory for saved Q-tables
â”‚   â””â”€â”€ .gitkeep
â””â”€â”€ exported_graphs/    # Directory for exported charts
    â””â”€â”€ .gitkeep
```

## ğŸ”¬ Expected Results

After training for **500-1000 episodes** with default parameters:

- **Episode Rewards**: Stabilizes around 95-100
- **Success Rate**: Reaches 85-95%
- **Steps to Goal**: Reduces to ~18-20 steps (near-optimal)
- **Q-Values**: Mean Q-value converges to positive values

**Optimal Path Length**: 18 steps (for 10x10 grid from top-left to bottom-right)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Q-Learning algorithm based on Watkins & Dayan (1992)
- Built with PyQt5 for cross-platform GUI
- Visualization inspired by OpenAI Gym environments

## ğŸ“§ Contact

**Pulkit Sulekh** - https://www.linkedin.com/in/pulkitsulekh

Project Link: https://github.com/pulkitsu/RL-Robot-Navigation-in-a-2D-Grid-System

---

â­ **Star this repo if you find it helpful!**
